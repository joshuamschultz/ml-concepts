{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is way of dealing with probabilities in identification. \n",
    "\n",
    "A deep learning model, or regression analysis done with the possibility of multiple outcomes, does not \"choose\" which outcome is being predicted. Rather, it estimates a probability for each outcome, and then takes the highest one.\n",
    "\n",
    "So if you are predicting digits, you are actually predicting the probability that the writing you are analyzing is equal to each possible digit (0,1,2,...) and then choosing the highest probability answer. \n",
    "\n",
    "Softmax is the mathmatical way we implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say each class is $k$. Under the assumption that we are classifying an iris flower, we have 3 *k*'s or possible classifications; Virginica, Versicolor, and Setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance of training data, we calculate a linear regression of that instance to each possible outcome. \n",
    "$$ s_k(\\mathbf{x}) = \\mathbf{x}^\\intercal\\boldsymbol{\\theta}^{(k)} $$\n",
    "\n",
    "This gives each potential class a score. However, these scores are just logit probabilities. So They will not add to 1. We want the probability that each class is correct, and so all classes should add to 1. This is what the softmax function does.\n",
    "\n",
    "$$ \\hat{P}_k = \\sigma(\\mathbf{s}(\\mathbf{x}))_k  \\frac{exp(s_k(\\mathbf{x}))}{\\sum^{K}_{j=1}exp(s_j(\\mathbf{x}))} $$\n",
    "\n",
    "Each score is divided by the sum of all scores, thereby making them \"percents\" and adding to 1. \n",
    "\n",
    "* $K$ is the number of classes\n",
    "* $\\mathbf{s}(\\mathbf{x})$ is the vector of scores for each class for the instance $\\mathbf{x}$\n",
    "* $\\sigma(\\mathbf{s}(\\mathbf{x}))_k$ is the predicted probability that $\\mathbf{x}$ belongs to class $k$, given $\\mathbf{s}(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Softmax regression classifier prediction is\n",
    "$$ \\hat{y} = argmax_k  \\sigma(\\mathbf{s}(\\mathbf{x}))_k $$\n",
    "\n",
    "$argmax$ is a way of saying \"return the value that maximizes the function.\n",
    "\n",
    "so this is \"return the k that maximizes the predicted probability that $\\mathbf{x}$ belongs to class $k$. Or which class is the higest probability choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function used to train models using probabilities and softmax is cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the famous flower classification example\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris['data'][:, (2,3)]\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = iris['target']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# LogisticRegression uses one vs the rest on default, so we will use multinomial \n",
    "# plus a solver that uses softmax such as 'lbfgs'\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test a pedal 5cm long and 2 cm wide.\n",
    "prediction = softmax_reg.predict([[5,2]])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginica'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target_names'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is \"virginica\". But we can see all the probabilities after the softmax function applied (and they should add to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.38014896e-07, 5.74929995e-02, 9.42506362e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5,2]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virginca = 94.25% \n",
    "Versicolor = 5.75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setosa': 6.380148956081825e-07,\n",
       " 'versicolor': 0.05749299952558009,\n",
       " 'virginica': 0.9425063624595242}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(iris['target_names'],softmax_reg.predict_proba([[5,2]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
