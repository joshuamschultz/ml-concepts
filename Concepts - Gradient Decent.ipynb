{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from [here](./Concepts%20-%20Linear%20Regression.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "m = 100 #observations\n",
    "\n",
    "X = 2 * np.random.rand(m,1)\n",
    "y = 4 + 3 * X + np.random.randn(m,1)\n",
    "bias_array = np.ones((m,1))\n",
    "X_b = np.c_[bias_array,X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we either don't have a [normal equation](./Concepts%20-%20Linear%20Regression.ipynb) to directly minimize a cost function for estimating theta, we may need to find it iteratively. This is gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we adjust $\\boldsymbol{\\theta}$, the error will decrease as we get closer to a min, and higher as we get further from the min error. Gradient Decent simply looks the rate of change, or slope (calculated as the partial derivative), and then takes one step in the direct that reduces the error the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best analogy is that you are decending a mountain on which you can only see 1 foot in front of you. You will look in all directions, and take a step in the direction that has the greatest decreased grade. If you continue this, you will eventually find your way to the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with this include:\n",
    "* this method may get you to a local min but not the global min\n",
    "* learning rate is important. If you too low, you'll never reach min. If too high you'll bounce around above it.\n",
    "\n",
    "*note* Learning rate is the size of the step you take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cost function is below:\n",
    "$$(\\frac{1}{m})\\sum_{i=1}^{m}(\\boldsymbol{\\theta}^\\intercal \\mathbf{x}^{i} - y^{i})^{2} $$  \n",
    "\n",
    "$$ MSE(\\boldsymbol{\\theta})$$\n",
    "\n",
    "Then the partial deravitive can be found with\n",
    "$$ \\frac{\\partial}{\\partial\\theta_{j}}MSE(\\boldsymbol{\\theta}) = (\\frac{2}{m})\\sum_{i=1}^{m}(\\boldsymbol{\\theta}^\\intercal \\mathbf{x}^{i} - y^{i}){x_j}^{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is for one direction, but you would need to create the full space around you to see which way is lowest. So you'll need to do for each possible direction to move theta (2 directions if $\\theta$ is a scalar, 4 directions if $\\boldsymbol{\\theta}$ is a vector with 2 scalars, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you end up with a vector of gradients.\n",
    "\n",
    "$$ \\mathbf{\\nabla_\\theta} MSE(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial}{\\partial\\theta_{0}}MSE(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial}{\\partial\\theta_{1}}MSE(\\boldsymbol{\\theta}) \\\\ \\vdots  \\\\ \\frac{\\partial}{\\partial\\theta_{n}}MSE(\\boldsymbol{\\theta}) \\end{bmatrix} =  \\frac{2}{m} \\mathbf{X}^{\\intercal} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient decent will show you the steepest part, so your next step is simply a *negative* of the gradient scaled by the *learning rate* or $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 #learning rate\n",
    "n_iterations = 1000 #number of steps before settling on min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02526395],\n",
       "       [0.04919516]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.random.rand(2,1) # start with a random weight, a randome place on the gradient space\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(n_iterations):\n",
    "    gradients = 2/m *X_b.T.dot(X_b.dot(theta)-y) #calculate the full vector space of partial derivatives (gradients)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24963579],\n",
       "       [2.81740108]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above theta is the same as was found using the normal equation [here](./Concepts%20-%20Linear%20Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equations use the entire training set to calculate each step. This can get computationally heavy. So you can use Stochastic Gradient Decent.\n",
    "\n",
    "SGD looks at random instances and calculates the partial derivative of just one instance. This means it is very fast, but jumps around a lot as the random variation in the dataset will cause the cost function to also jump around a bit.\n",
    "\n",
    "Because of how much the cost function will jump, SGD often reduces its learning rate as it gets closer, taking smaller and smaller steps. Overtime this means it will settle around a min rather than continually jumping in equal steps around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 #These are the parameters used for the learning schedule\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta)-yi)\n",
    "        eta = learning_schedule(epoch * m+ i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19428856],\n",
       "       [2.8757777 ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.1, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty=None, power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.20455032]), array([2.76081505]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
